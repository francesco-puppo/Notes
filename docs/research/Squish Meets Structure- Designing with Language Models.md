---
id: f4ff7038-40e7-4c15-ac81-51a6748e6636
omnivore_error: There was an error parsing the front matter template. See console for details.
---

# Squish Meets Structure: Designing with Language Models

[Read on Omnivore](https://omnivore.app/me/squish-meets-structure-designing-with-language-models-18bcca20954) | [Read Original](https://maggieappleton.com/squish-structure)

## Highlights

> We’re trying to make an unpredictable and opaque system adhere to our rigid expectations for how computers behave.

> We currently have a mismatch between our old and new mental models for computer systems.

> The selling point of computers up until this point is they’re incredibly predictable, structured, and reliable.

> In the traditional computer world, you press a button and a predictable series of logic functions execute
> 
> We can see exactly which functions are running.

> But the thing is... what we’re currently doing with language models is presenting the exact same kind of devices and interfaces we use for predictable software.

> Predictability is supposed to be a hallmark of good user experience.
> 
> We judge an interface as “good” if the user knows what to expect, and is never faced with results or feedback that seem out of the blue or confusing to them.
> 
> I still think this is true, but maybe we’ll have to rethink this as we start to use more generative and emergent systems in our work. If the system is unpredictable by nature, we'll need new interaction patterns to accommodate that.

> Predictability is supposed to be a hallmark of good user experience.
> 
> We judge an interface as “good” if the user knows what to expect, and is never faced with results or feedback that seem out of the blue or confusing to them.
> 
> I still think this is true, but maybe we’ll have to rethink this as we start to use more generative and emergent systems in our work. If the system is unpredictable by nature, we'll need new interaction patterns to accommodate that.

> This approach is also sometimes called “prompt chaining” – as in you make chains of commands that include prompts to language models.

Used to try and control how the AI will look for its answer

> I see lots of products that try to outsource too much cognition to both language models and users. Most look something like this.
> 
> They present you with a “Magic AI” input claiming it can do anything you want.

> This thing has no affordances! There are no knobs or door handles on this thing. This interface offloads a ton of cognitive labour to the user.
> 
> _You_ have to figure out what it’s capable of doing because the designers of this system certainly haven't done it for you.
> 
> _You_ also have to figure out how to get a good result out of it. Good luck learning prompt engineering while you're at it.
> 
> The problem here is this interface _can’t_ actually do everything.

> Instead of making generic interfaces and leaving the user to come up with their own solutions, we can instead design language models that give users a set of specific tools.
> 
> We should make tiny, sharp, specific tools with models.

